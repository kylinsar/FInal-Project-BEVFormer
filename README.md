# Abstract
This project is based on the ECCV 2022 paper BEVFormer[1]. It's a novel approach for autonomous driving perception tasks that utilizes unified Bird's-Eye-View (BEV) representations learned through spatiotemporal transformers. This framework effectively combines spatial and temporal information, interacting with these domains via grid-shaped BEV queries. We will initially attempt to replicate the core content of this project on the mini nuScenes dataset, implementing and rendering the output results to evaluate the model's performance using the NDS metric. During this phase, the concept of quaternions is crucial to our project, and we plan to create a demo to visualize this process. In the later stages of the project, we will integrate the BEVFormer algorithm into the EasyCV open-source framework, focusing on code optimization in terms of training speed and algorithmic convergence. Additionally, we aim to further enhance the model's performance by utilizing the inference optimization tool PAI-Blade.
